import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import re


async def parse_restaurant_info(md_file_path, output_file_path):
    """
    –§–∏–Ω–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä, –∏–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ + –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞, –æ–±—Ö–æ–¥—è cloudflare.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
    md_file_path : –ü—É—Ç—å –∫ md —Ñ–∞–π–ª—É —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤.
    output_file_path : –ü—É—Ç—å –∫ md —Ñ–∞–π–ª—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
    """
    restaurant_links = read_restaurant_links_from_md(md_file_path)

    restaurant_info = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(
            executable_path=r"C:\Program Files\Google\Chrome\Application\chrome.exe",
            headless=True
        )

        for index, link in enumerate(restaurant_links[:25], start=1):
            try:
                print(f"üîç –û—Ç–∫—Ä—ã–≤–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É: {link}")

                context = await browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36",
                    extra_http_headers={
                        "Accept-Language": "en-US,en;q=0.9"
                    }
                )
                page = await context.new_page()

                if await load_page_with_retry(page, link):
                    await page.wait_for_selector('a[href^="tel:"]', timeout=50000)

                    html = await page.content()
                    soup = BeautifulSoup(html, "html.parser")

                    name_tag = soup.find("a", {"style": "opacity: 1;"})
                    name = name_tag.get_text(strip=True) if name_tag else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

                    phone_tag = soup.find("a", href=re.compile(r"tel:\+?\d+"))
                    phone_number = phone_tag.get_text(strip=True) if phone_tag else "–ù–µ –Ω–∞–π–¥–µ–Ω"

                    print(f"üìû –ù–∞–π–¥–µ–Ω –Ω–æ–º–µ—Ä: {phone_number}, –ù–∞–∑–≤–∞–Ω–∏–µ: {name}")
                    restaurant_info.append(f"{index}. {name} : {phone_number}")
                    await page.close()
                else:
                    print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {link} –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫.")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {link}: {e}")

        await browser.close()

    save_restaurant_info_to_md(output_file_path, restaurant_info)


def save_restaurant_info_to_md(output_file_path, restaurant_info):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞—Ö –≤ markdown —Ñ–∞–π–ª.
    """

    with open(output_file_path, "w", encoding="utf-8") as file:
        file.write("# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞—Ö:\n\n")
        for line in restaurant_info:
            file.write(f"{line}\n")


async def load_page_with_retry(page, url, retries=3):
    """
    –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–≤—Ç–æ—Ä–æ–º –ø—Ä–∏ –æ—à–∏–±–∫–µ.
    """

    for attempt in range(retries):
        try:
            print(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –¥–ª—è {url}")
            await page.goto(url, timeout=70000)
            await page.wait_for_selector('a[href^="tel:"]', timeout=50000)
            return True
        except Exception as e:
            print(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å –¥–ª—è {url}: {e}")
            await asyncio.sleep(5)
    return False


def read_restaurant_links_from_md(md_file_path):
    """
    –ß–∏—Ç–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤ –∏–∑ markdown —Ñ–∞–π–ª–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: –°–ø–∏—Å–æ–∫ URL-–æ–≤ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤.
    """

    links = []
    with open(md_file_path, "r", encoding="utf-8") as file:
        for line in file:
            match = re.search(r"\[(?:–°—Å—ã–ª–∫–∞|.*?)\]\((https://restaurantguru\.ru/.*?)\)", line)
            if match:
                links.append(match.group(1))
    return links
